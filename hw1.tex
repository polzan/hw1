\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{xr}
\usepackage[section]{placeins}
%\usepackage{hyperref}

\externaldocument{hw1_code}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{\lstinputlisting[caption={\ttfamily #1.m},
    label={lst:#1}]{matlab/#1.m}}
\newcommand{\inlinecode}[1]{\lstinline[basicstyle=\ttfamily,
    keywordstyle={},
    stringstyle={},
    commentstyle={\itshape}]{#1}}

\renewcommand{\vec}[1]{\underline{#1}}

\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\abs}[1]{\left|#1\right|}

\author{Enrico Polo \and Riccardo Zanol}
\title{Homework 1}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{Problem 1}
Before describing the results we obtained solving this point, we want to briefly describe some useful functions we used several times to solve the homework:
\begin{itemize}
 \item the function \inlinecode{generate_x} (Lst.~\ref{lst:generate_x}) generates a realization of the required random process, given the variance of the Gaussian noise and the length of the realization (in number of samples) we want to consider. We can also control the realization we are observing through the parameter \inlinecode{'seed'} that is useful if we want to compare the results of the different settings for a specific realization; 
 \item the function \inlinecode{autocorrelation_unbiased} (Lst.~\ref{lst:autocorrelation_unbiased}) returns an unbiased estimator of the process autocorrelation given a process realization, by applying $\hat{r}_x(n) = \frac{1}{K-n} \sum_{k=n}^{K-1} {x(k)x^*(k-n)}$ for $n = 0,1,..N$ where N is the maximum lag we want to estimate ($0\leq N \leq K-1$);
 \item the function \inlinecode{autocorrelation_biased} (Lst.~\ref{lst:autocorrelation_biased}) returns a biased estimator of the same quantity, starting from the unbiased one, through the relation $\breve{r}_x(n) = \bigl( 1 - \frac{|n|}{K}\bigr) \hat{r}_x(n)$ for $n = 0,1,..N$;
 \item  the function \inlinecode{theoretical_autocorr} (Lst.~\ref{lst:theoretical_autocorr}) gives us the values of the autocorrelation function obtained by analysis $r_x (n) =  \E{x(k)x^*(k-n)}$ that in our case is: $r_x (n) = e^{j2 \pi f_1n} + e^{j2\pi f_2n} + \sigma_w^2 \delta_n$.
 \item the function \inlinecode{ar_model} (Lst.~\ref{lst:ar_model})
   takes in input a realization of the process, the order of the
   filter and the kind of autocorrelation estimator we want to use, it
   solves the Yule-Walker equation
   \[ R\vec{a} = -\vec{r} \]
   and estimates the power of the white noise which is is the input of
   the filter $\frac{1}{A(z)}$ in the AR model of the process using the equation
   \[ \sigma^2_w = r_x(0) + \vec{r}^H\vec{a} . \]
   Finally it returns the filter coefficients and the estimated $\sigma^2_w$.
\end{itemize}
To solve Problem 1 we firstly computed our reference PSD by using the \inlinecode{theoretical_psd} (Lst.~\ref{lst:theoretical_psd}) function. It computes the PSD as the sum of the constant $\sigma_w^2$ (PSD of the white noise, given in input) and the two spectral lines in $f1$ and $f2$ (replaced by triangles of height $K$ and base $2K$ as required). In our settings we consider $T_c = F_s = 1$.
%\newline In order to estimate the PSD by anaysing a realization of the process we have %used an unbiased estimator $\hat{r}_x(n)$ because it is the one with the smaller variance %and, by testing, it give us the better results.
\begin{itemize}
\item[a)] For the Correlogram method, we implement the relation
  \[ P_{cor}(f) = T_c \sum_{k=-L}^{L} w(k)\hat{r}(k)e^{-j 2 \pi  f k T_c} . \]
  To reduce the variance of our estimator we set $L=\frac{K}{5}$ and considered only the first 200 samples. Than by symmetry we found the values of $\hat{r}(n)$ also for negative lags (setting $true$ the value of the last optional parameter of \inlinecode{autocorrelation_unbiased}). In our tests we have obtained good results by using the rectangular window, so we choose this one because it has the narrowest central lobe and so it gives the best frequency resolution.
 \item[b)] To obtain the Periodogram we just apply
   \[ P_{PER} = \frac{1}{K T_c} |\tilde{X}(f)|^2 \]
   to the whole realization, with $\tilde{X}(f)$ obtained doing the fft of $\hat{r}_x(n)$.
 \item[c)] For the Welch method we have implemented the \inlinecode{psd_welch_estim} (Lst.~\ref{lst:psd_welch_estim}) function. The input parameters we need to set correctly in order to reach the best results are:
 \begin{itemize}
 \item the kind of window we want to use $w$,
 \item the length of each sub-sequence we are considering $D$, 
 \item the number of samples we want to overlap between two consecutive sequences $S$. 
\end{itemize} 
The function uses the $D$ and $S$ values to find the number of segments $N_s = \bigl\lfloor \frac{K-D}{D-S} -1\bigl\rfloor $ in which we are dividing our signal. Computes the normalized energy of the chosen window $M_w = \frac{1}{D} \sum_{k=1}^{D} {\abs{w(k)}^2} $ and for each segment (s) computes the Periodogram $P_{PER}^{(s)} = \frac{1}{D T_c M_w} |\tilde{X}(f)|$. Stored the value of each $P_{PER}^{(s)}$ in the columns of an auxiliary matrix we finally do the mean between columns to finally get $P_{we}(f) = \frac{1}{N_s} \sum_{s=0}^{N_s-1} P_{PER}^{(s)} $
\newline Doing some tests we find that a suitable value of $D$ is 100 samples and for $S$ is 25 samples. With this choice we have enough samples per segment and enough segments to perform a useful mean between the $P_{PER}^{(s)}$.
\newline Also in this case we choose a rectangular window (like in point a) because is the one with the narrowest central lobe in the frequency domain.
\item[d)]For the AR model the main function we have used the is \inlinecode{psd_ar_estim} (Lst.~\ref{lst:psd_ar_estim}). It computes the AR filter coefficients  and estimate the power of the needed input noise by using \inlinecode{ar_model} (Lst.~\ref{lst:ar_model}). This last function requires the process $x$, the order of the AR filter we want to use $N$ and the kind of correlation estimator we want to use (we must give also the value of $\sigma_w^2$ to use the theoretical autocorrelation $r_x (n)$). Once estimated the autocorrelation function for lags from 0 to N samples, it computes the relative $NxN$ autocorrelation matrix $R$ (lags from 0 to N-1), the vector $\vec{r} = [r_x (1), r_x (2), ...,r_x (N)]^T$ and solve the system $R \vec{a} = - \vec{r}$. Finally it computes the noise power $\sigma_w^2$ by the relation $\sigma_w^2 = r_x(0) + \vec{r}^H \vec{a}$. In this case we started from a realization of the process so we used an unbiased estimator $\hat{r}(n)$ instead of $r_x (n)$. 
\newline Then the main function computes the frequency response of the filter $A(f)$ and estimates the PSD of the process through $P_{ar} = \sigma_w^2 T_c \frac{1}{|A(f)|^2}$ (also in this case we consider $T_c=1$).
\newline In our tests we tried to find the best order $N$ of the filter. We find that an order two filter should be enough because we are estimating the PSD of a process that contains only two complex exponentials and white noise, but in this case we find that the configuration with $N = 4$  is the one that best approximates the ideal shape.
\end{itemize}
In Fig. ~\ref{plot:psd_comparison_126} we can see the comparison between the estimated PSDs choosing for each method the parameters we have reported above and the ideal one.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/psd_comparison_126}
  \caption{PSDs comparison ($\sigma_w^2 = 1.26$)}
    \label{plot:psd_comparison_126}
\end{figure}
\section*{Problem 2}
For problem two the functions used are the same, but because we have a significant lower value of $\sigma_w^2 = 0.0002 << 1.26$ we have to find a new optimal configuration. In particular while point b) is still the same of the previous point (it have no parameters to set), we need to find a proper setting for the Correlogram, the Welch and AR methods:
\begin{itemize}
 \item[a)] in the Correlogram method we need to change the window we are using. In fact by using a rectangular window like in Problem 1 we notice that between the two spectral lines the PSD is much higher then it would be. So we used a window that decreases faster to zero (in frequency domain) and we have good results with the Hanning and the Blackman. We choose the Hanning because is better with regards of the frequency resolution. Unfortunately this windowing choice leaves the PSD quite higher than the theoretical value between the two peaks.
 \item[c)] for the Welch method, by testing, we don't find relevant differences by changing the values of $D$ or $S$. For the choice of the window we pick an Hanning window for the same reason of point a).
 \item[d)] For the AR model we tried to increase $N$ in order to obtain better results (compared to the ideal PSD) but we find that for $N\geq 3$ the $P_{ar}$ starts to show some peaks that are not due to the signal. In fact recalling the formula $P_{ar} = \sigma_w^2 T_c \frac{1}{|A(f)|^2}$ we see that the system may become unstable if the magnitude of the zeros of $A(f)$ isn't smaller then 1. It happens for $N\geq3$, so in this point we choose $N=2$. 
\end{itemize}
In Fig. ~\ref{plot:psd_comparison_002} are plotted the PSDs we have obtained with this new configuration.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/psd_comparison_002}
  \caption{PSDs comparison ($\sigma_\omega^2 = 0.0002$)}
  \label{plot:psd_comparison_002}
\end{figure}
\section*{Problem 3}
To solve this problem we use the function \inlinecode{predictor} (Lst.~\ref{lst:predictor}). This function calls the function \inlinecode{ar_model} with the attribute '$theoretical$' in order to compute the ideal coefficients of the AR(N) model of our process. So we get $ \vec{a} = [1, a_1, a_2,...a_N]^T $, and by using the relation for the optimum predictor $ \vec{a} = [1,-\vec{c}_{opt}]^T $ we find $ \vec{c}_{opt} = -[a_1, a_2,...a_N]^T$. The parameter we need to find in this case is ($N$) the order of the predictor. We started from $N=2$ and find $J_{min} = 6.1832\cdot10^{-4}$, than for $N=3$ we find $J_{min} = 3.3586\cdot10^{-4}$ that is a half of the previous value. If we try to increase more the value of $N$ the error $J_{min}$ does not decrease anymore, so we decided to stop to $N=3$ because we don't have a significant improvement of the performances of our system by increasing it's complexity.
With this configuration we find: $c_{opt_1} = 0.3588 -j0.0862$,  $c_{opt_2} = -0.2616 +j0.1333$ and $c_{opt_3} = -0.5139 +j0.4390$.
\newline In Fig.~\ref{plot:predictor_zp} are plotted the zeros and the poles of the predictor error filter $A(z) = 1+a_1 z^{-1} + a_2 z^{-2} + a_3 z^{-3}$.
We can see that two zeros of the filter (the one in the unit circle) are deleting the exponential part of the process, like we expect. 
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/predictor_zp}
  \caption{Plot of the zeros and poles of the predictor error filter}
  \label{plot:predictor_zp}
\end{figure}
\section*{Problem 4}
To implement the LMS predictor we use the algorithm described in
section 3.1.2 of the textbook. The function \inlinecode{lms_predictor}
(Lst.~\ref{lst:lms_predictor}) starts at time $k=N-1$, because it
needs $N$ samples from the input signal $x(k)$, and at each
time instant:
\begin{itemize}
  \item extracts from $x(k)$ the vector $\vec{x}_k = [x(k),
    x(k-1),\dots,x(k-N+1)]^T$ that is the input of the adaptive filter
    at time $k$
  \item computes the filter output $y(k) = \vec{x}_k^H\vec{c}(k)$
  \item computes the error $e(k) = d(k) - y(k)$ where $d(k) = x(k+1)$
    since we want the filter to predict the next sample of the random
    process $x(k)$
  \item computes the filter coefficients that will be used at the next
    iteration $\vec{c}(k+1) = \vec{c}_k + \mu e(k) \vec{x}_k^*$
\end{itemize}
This is repeated until time $k = K - 2$ since the last known sample of
the input $x(K-1)$ is needed to compute the value of the error
function $e(K-2)$.

The parameters of the LMS algorithm are the order of the filter $N$
and the adaptation gain $\mu$ and for the first one we choose $N = 3$
because we want to compare the results with the ones from problem 3,
since the coefficients $\vec{c}_{opt}$ of the Wiener filter should be
the values that the coefficients $\vec{c}(k)$ of the LMS filter
converge to.  The second parameter $\mu$ must be chosen in the
interval $\left[0,\frac{2}{Nr_x(0)}\right]$, where we use the
theoretical power of the input signal $r_x(0) = 2 + \sigma^2_w$ , in
order to ensure the convergence of $\vec{c}(k)$ and $e(k)$ in the mean
sense and in the mean square sense,
%% \begin{align*}
%%   \E{\vec{c}(k)} &\rightarrow \vec{c}_{opt} &\text{as} \quad k \rightarrow \infty \\
%%   \E{e(k)} &\rightarrow 0  &\text{as} \quad k \rightarrow \infty \\
%%   \\
%%   \E{\norm{\vec{c}(k)-\vec{c}_{opt}}^2} &\rightarrow \text{constant} &\text{as} \quad k \rightarrow \infty \\
%%   \E{\abs{e(k)}^2} &\rightarrow \text{constant} &\text{as} \quad k \rightarrow \infty 
%% \end{align*}
with the same unbiased estimator we used before.  We tried the choice
that yields the fastest rate of convergence of $J(k)$ (when $J(k) \gg
J_{min}$) from equation (3.121) of the textbook: $\mu_{opt} =
\frac{1}{Nr_x(0)}$. This value of $\mu$ does make the LMS algorithm
converge very quickly but with a huge misadjustment: the MSD
$\frac{J(\infty) - J_{min}}{J_{min}} = 0.997$, a smaller value, like
$\tilde{\mu} = 0.175$, provides a good trade-off between speed of
convergence and MSE and allows us to have a misadjustment below 10\%
(see Fig.~\ref{plot:lms_mse}).

The input signal $x(k)$ is generated with the same seed value of
problem 3 for the random number generator so it is the same
realization.

After running the LMS predictor we get the following filter
coefficients at the last iteration
\begin{align*}
  c_1(K-1) &= 0.3579 - 0.0856j   \\
  c_2(K-1) &=  -0.2610 + 0.1326j \\
  c_3(K-1) &= -0.5139 + 0.4384j
\end{align*}
these values are very close to the ones we computed using the
Wiener-Hopf solution at problem 3. The real and imaginary parts of the
first two coefficients $c_1(k)$ and $c_2(k)$ are plotted as $k$ varies
in Fig.~\ref{plot:coeff_lms_c1} and Fig.~\ref{plot:coeff_lms_c2} where
we can notice that after $k \approx 80$ samples they are practically
at their convergence values.
%% \begin{table}[h]
%%   \centering
%%   \begin{tabular}{>{$}c<{$}>{$}c<{$}>{$}c<{$}}
%%     \text{Coefficient} & \text{Real part} & \text{Imaginary part} \\
%%     \hline
%%     c_0 & 1.0071 & - 0.2479j \\
%%     c_1 & -0.8942 & 0.4401j
%%   \end{tabular}
%%   \caption{Values of the LMS predictor coefficients at convegence}
%%   \label{tab:coeff_lms}
%% \end{table}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c1}
  \caption{Real and imaginary parts of $c_1(k)$}
  \label{plot:coeff_lms_c1}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c2}
  \caption{Real and imaginary parts of $c_2(k)$}
  \label{plot:coeff_lms_c2}
\end{figure}

In Fig.~\ref{plot:lms_mse} we compare the prediction error function
$e(k)$ obtained from this realization of $x(k)$ with an average over
200 other realizations of the same process.  Along with the square
error $\abs{e(k)}^2$ and the MSE $\E{ \abs{e(k)}^2 }$ we plotted the
minimum MSE $J_{min} = \sigma^2_x + \vec{r_N}^Hc_{opt}$ obtained when
using the Wiener-Hopf solution in problem 3 and the value that we
expect as $k$ goes to infinity from equation (3.115) of the textbook:
\[ J(\infty) = \frac{2}{2-\mu N r_x(0)}J_{min} \]
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/lms_mse}
  \caption{Square error and MSE of the LMS predictor}
  \label{plot:lms_mse}
\end{figure}
From this plot we can see that the MSE of the predictor converges to a
constant value $J(\infty)$ close to the minimum possible MSE $J_{min}$
obtainable by a linear predictor, the MSD is now $0.096$.
%% in accordance with
%% what we would expect from equation (3.117) of the textbook:
%% \[ J_{ex}(\infty) \approx \frac{\mu}{2}Nr_x(0)J_{min}  . \]
\section*{Problem 5}
To determine if a random process contains spectral lines we can try to
model it as an AR process and determine the lines' frequencies from
the poles of the transfer function
\[ H(z) = \frac{1}{A(z)} = \frac{1}{\sum_{k=0}^{N-1}a_kz^{-k}} \]
of the filter that generates the process $x(k)$ from white
noise. Since the amplitude of the lines is very high, they will
correspond to poles of $H(z)$ close to the unit circle, while the
other poles will have a smaller magnitude (unless the PSD of the
broadband component of $x(k)$ has an amplitude comparable to the one
of the lines). After we have identified the poles associated to the
spectral lines we can obtain the frequencies from the phases
$\theta_i$ of each pole, translated into the $[0,2\pi]$ interval, as
$f_i = \frac{\theta_i}{2\pi}F_s$.

Since the Wiener-Hopf solution for the predictor filter and the
Yule-Walker equation that gives us the solution for the AR coefficients
are similar:
\begin{align*}
  R_N\vec{c}_{opt} &= \vec{r_N} \\
  R\vec{a} &= -\vec{r}
\end{align*}
we can approximate the coefficients $\vec{c}_{opt}$ using the same LMS
algorithm used in problem 4 and then get the AR coefficients as $
\vec{a} = -\vec{c}_{opt}$. In this case we do not care about the power
of the white noise input of the AR filter since we only need the
position of the poles of $H(z)$.

The order of the LMS filter $N$ must be chosen to be at least equal to
the number of spectral lines present in $x(k)$ to detect all of
them. In our case $N=2$ would be enough, but we try with $N=32$ to see
where the other poles end up. This time we do not care about the value
at convergence of $\E{\abs{e(k)}^2}$, but we do want the mean square
error of the coefficients $ \E{\norm{\Delta\vec{c}}^2} $ to be small,
so the choice of $\tilde{\mu}$ must take into account that, according
to equation (3.81) of the textbook extended to the general case, 
\[ \E{\norm{\Delta\vec{c}}^2} \approx \frac{\mu}{2}J_{min} . \]
If we use as input a realization of the same process of problems 2-4
then $J_{min} = 1.7\cdot10^{-4}$ and we use an adaptive gain with
$\tilde{\mu} = 1$, so that we maximize the speed of convergence, we
get an MSE on the coefficients with an order of magnitude of $10^{-6}$,
so we can use this value for $\tilde{\mu}$.

If we consider each pole to be on the unit circle when its magnitude
$|p_i|$ is in the interval $[1-10^{-2},1+10^{-2}]$ we can identify the
frequencies of the two complex exponentials in the signal $x(k)$
(\SI{0.125}{\Hz} and \SI{0.8}{\Hz}). In Fig.~\ref{plot:part5_poles} we
can see the poles of the transfer function $H(z)$.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/part5_poles}
  \caption{Poles of the AR filter}
  \label{plot:part5_poles}
\end{figure}

When we increase the order $N$ beyond what is needed to model the
signal we notice that the additional poles tend to move toward the
unit circle as $N$ increases and at a certain point they fall below
the threshold we have set and non-existent spectral lines are detected
by the algorithm. The magnitude of the poles associated to the true
spectral lines also tends to get closer and closer to one, so it may
be possible to adjust the threshold as N increases.
\end{document}
