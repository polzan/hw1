\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{xr}
\usepackage[section]{placeins}
%\usepackage{hyperref}

\externaldocument{hw1_code}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{\lstinputlisting[caption={\ttfamily #1.m},
    label={lst:#1}]{matlab/#1.m}}
\newcommand{\inlinecode}[1]{\lstinline[basicstyle=\ttfamily,
    keywordstyle={}]{#1}}

\renewcommand{\vec}[1]{\underline{#1}}

\newcommand{\E}[1]{\operatorname{E}\left[#1\right]}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\newcommand{\abs}[1]{\left|#1\right|}

\author{Enrico Polo \and Riccardo Zanol}
\title{Homework 1}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{Problem 1}
\section*{Problem 2}
\section*{Problem 3}
\section*{Problem 4}
To implement the LMS predictor we use the algorithm described in
section 3.1.2 of the textbook. The function \inlinecode{lms_predictor}
(Lst.~\ref{lst:lms_predictor}) starts at time $k=N-1$, because it
needs $N$ samples from the input signal $x(k)$, and at each
time instant:
\begin{itemize}
  \item extracts from $x(k)$ the vector $\vec{x}_k = [x(k),
    x(k-1),\dots,x(k-N+1)]^T$ that is the input of the adaptive filter
    at time $k$
  \item computes the filter output $y(k) = \vec{x}_k^H\vec{c}(k)$
  \item computes the error $e(k) = d(k) - y(k)$ where $d(k) = x(k+1)$
    since we want the filter to predict the next sample of the random
    process $x(k)$
  \item computes the filter coefficients that will be used at the next
    iteration $\vec{c}(k+1) = \vec{c}_k + \mu e(k) \vec{x}_k^*$
\end{itemize}
This is repeated until time $k = K - 2$ since the last known sample of
the input $x(K-1)$ is needed to compute the value of the error
function $e(K-2)$.

The parameters of the LMS algorithm are the order of the filter $N$
and the adaptation gain $\mu$ and for the first one we choose $N = 2$
because we want to compare the results with the ones from problem 3,
since the coefficients $\vec{c}_{opt}$ of the Wiener filter should be
the values that the coefficients $\vec{c}(k)$ of the LMS filter
converge to.  The second parameter $\mu$ must be chosen in the
interval $\left[0,\frac{2}{Nr_x(0)}\right]$, where we use the
theoretical power of the input signal $r_x(0) = 2 + \sigma^2_w$ , in
order to ensure the convergence of $\vec{c}(k)$ and $e(k)$ in the mean
sense and in the mean square sense,
%% \begin{align*}
%%   \E{\vec{c}(k)} &\rightarrow \vec{c}_{opt} &\text{as} \quad k \rightarrow \infty \\
%%   \E{e(k)} &\rightarrow 0  &\text{as} \quad k \rightarrow \infty \\
%%   \\
%%   \E{\norm{\vec{c}(k)-\vec{c}_{opt}}^2} &\rightarrow \text{constant} &\text{as} \quad k \rightarrow \infty \\
%%   \E{\abs{e(k)}^2} &\rightarrow \text{constant} &\text{as} \quad k \rightarrow \infty 
%% \end{align*}
with the same unbiased estimator we used before.  We tried the choice
that yields the fastest rate of convergence of $J(k)$ (when $J(k) \gg
J_{min}$) from equation (3.121) of the textbook: $\mu_{opt} =
\frac{1}{Nr_x(0)}$. This value of $\mu$ does make the LMS algorithm
converge very quickly but with a huge misadjustment: the MSD
$\frac{J(\infty) - J_{min}}{J_{min}} = 0.997$, a smaller value, like
$\tilde{\mu} = 0.175$, provides a good tradeoff between speed of
convergence and MSE and allows us to have a misadjustment below 10\%
(see Fig.~\ref{plot:lms_mse}).

The input signal $x(k)$ is generated with the same seed value of
problem 3 for the random number generator so it is the same
realization.

After running the LMS predictor we get the following filter
coefficients at the last iteration
\begin{align*}
  c_1(K-1) &= 1.0151 - 0.2443j   \\
  c_2(K-1) &= -0.8909 + 0.4525j
\end{align*}
these values are very close to the ones we computed using the
Wiener-Hopf solution at problem 3. The real and imaginary parts of the
two coefficients $c_1(k)$ and $c_2(k)$ are plotted as $k$ varies in
Fig.~\ref{plot:coeff_lms_c1} and Fig.~\ref{plot:coeff_lms_c2} where we
can notice that after $k \approx 100$ samples they are practically at
their convergence values.
%% \begin{table}[h]
%%   \centering
%%   \begin{tabular}{>{$}c<{$}>{$}c<{$}>{$}c<{$}}
%%     \text{Coefficient} & \text{Real part} & \text{Imaginary part} \\
%%     \hline
%%     c_0 & 1.0071 & - 0.2479j \\
%%     c_1 & -0.8942 & 0.4401j
%%   \end{tabular}
%%   \caption{Values of the LMS predictor coefficients at convegence}
%%   \label{tab:coeff_lms}
%% \end{table}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c1}
  \caption{Real and imaginary parts of $c_1(k)$}
  \label{plot:coeff_lms_c1}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c2}
  \caption{Real and imaginary parts of $c_2(k)$}
  \label{plot:coeff_lms_c2}
\end{figure}

In Fig.~\ref{plot:lms_mse} we compare the prediction error function
$e(k)$ obtained from this realization of $x(k)$ with an average over
200 other realizations of the same process.  Along with the square
error $\abs{e(k)}^2$ and the MSE $\E{ \abs{e(k)}^2 }$ we plotted the
minimum MSE $J_{min} = \sigma^2_x + \vec{r_N}^Hc_{opt}$ obtained when
using the Wiener-Hopf solution in problem 3 and the value that we
expect as $k$ goes to infinity from equation (3.115) of the textbook:
\[ J(\infty) = \frac{2}{2-\mu N r_x(0)}J_{min} \]
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/lms_mse}
  \caption{Square error and MSE of the LMS predictor}
  \label{plot:lms_mse}
\end{figure}
From this plot we can see that the MSE of the predictor converges to a
constant value $J(\infty)$ close to the minimum possible MSE $J_{min}$
obtainable by a linear predictor, the MSD is now $0.096$.
%% in accordance with
%% what we would expect from equation (3.117) of the textbook:
%% \[ J_{ex}(\infty) \approx \frac{\mu}{2}Nr_x(0)J_{min}  . \]
\section*{Problem 5}
To determine if a random process contains spectral lines we can try to
model it as an AR process and determine the lines' frequencies from
the poles of the transfer function of the filter that generates the
process from white noise. Since the lines have very high amplitude,
they will correspond to some poles that are close to the unit circle,
while the other poles will have a smaller magnitude (unless the PSD of
the broadband component has an amplitude comparable to the one of the
lines). After we have identified the poles associated to the spectral
lines we can obtain the frequencies $f_i$ from the phases $\theta_i$
of each pole traslated into the $[0,2\pi]$ interval: $f_i =
\frac{\theta_i}{2\pi}F_s$.

Since the Wiener-Hopf solution for the predictior filter and the
Yule-Walker equation that gives us the solution for the AR coefficients
are similar:
\[ R\vec{a} = -\vec{r} \]
we can approximate the coefficients $\vec{c}_{opt}$ using the same LMS
algorithm used in problem 4 and get the AR coefficients as $ \vec{a} =
-\vec{c}_{opt}$. In this case we do not care about the power of the
white noise input of the AR filter since we only need the position of
the poles of the filter.

The order of the LMS filter $N$ must be chosen to be at least equal to
the number of spectral lines present in $x(k)$ to detect all of
them. In our case $N=2$ would be enough, but we try with $N=64$ to see
where the other poles end up. This time we do not care about the value
at convergence of $\E{\abs{e(k)}^2}$, {\color{red} but we do about the
  error of the coefficients $\E{\abs{\Delta}}$adaptive gain of the LMS
  filter $\tilde{\mu} = 0.175$}

If we consider each pole to be on the unit circle when its magnitude
$|p_i|$ is in the interval $[1-10^{-2},1+10^{-2}]$ we can identify the
frequencies of the two complex exponentials in the signal $x(k)$. In
Fig.~\ref{plot:part5_poles} we can see the poles of the transfer
function.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/part5_poles}
  \caption{Poles of the AR filter}
  \label{plot:part5_poles}
\end{figure}

When we increase the order $N$ beyond what is needed
to model the signal we notice that the additional poles tend to move
toward the unit circle as $N$ increases and at a certain point they
fall below the threshold we have set and non-existent spectral lines
are detected by the algorithm. The magnitude of the poles associated
to the true spectral lines also tends to be more close to one, so it
may be possible to adjust the threshold as N increases. {\color{red}
  proviamo a fare i conti a vedere se viene una relazione da usare
  nell'algoritmo?}
\end{document}
