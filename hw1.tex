\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{listings}
\usepackage{color}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{xr}
%\usepackage{hyperref}

\externaldocument{hw1_code}

\definecolor{matlabgreen}{RGB}{28,172,0}
\definecolor{matlablilas}{RGB}{170,55,241}

\newcommand{\includecode}[1]{
  \lstinputlisting[caption={\ttfamily #1.m}, label={lst:#1}]{matlab/#1.m}
}
\newcommand{\inlinecode}[1]{
  \lstinline[basicstyle=\ttfamily,keywordstyle={}]{#1}
}

\renewcommand{\vec}[1]{
  \underline{#1}
}

\newcommand{\E}[1]{
  \operatorname{E}\left[ #1 \right]
}

\newcommand{\norm}[1]{
  \left \lVert #1 \right \rVert
}

\newcommand{\abs}[1]{
  \left| #1 \right|
}

\author{Enrico Polo \and Riccardo Zanol}
\title{Homework 1}

\begin{document}
\lstset{
  language=Matlab,
  basicstyle={\ttfamily \footnotesize},
  breaklines=true,
  morekeywords={true,false,warning,xlim,ylim},
  keywordstyle=\color{blue},
  stringstyle=\color{matlablilas},
  commentstyle={\color{matlabgreen} \itshape},
  numberstyle={\ttfamily \tiny},
  frame=leftline,
  showstringspaces=false,
  numbers=left,
  upquote=true,
}
\maketitle
\section*{Problem 1}
\section*{Problem 2}
\section*{Problem 3}
\section*{Problem 4}
To implement the LMS predictor we use the algorithm described in
section 3.1.2 of the textbook. The function \inlinecode{lms_predictor}
(Lst.~\ref{lst:lms_predictor}) starts at time $k=N-1$, because it
needs $N$ samples from the input signal $x(k)$, and at each
time instant:
\begin{itemize}
  \item extracts from $x(k)$ the vector $\vec{x}_k = [x(k),
    x(k-1),\dots,x(k-N+1)]^T$ that is the input to the Wiener filter
    at time $k$ {\color{red} si chiama veramente cosÃ¬ anche questo?}
  \item computes the filter output $y(k) =
    \sum_{n=0}^{N-1}c_n(k)x(k-n) = \vec{x}_k^H\vec{c}(k)$
  \item computes the error $e(k) = d(k) - y(k)$ where $d(k) = x(k+1)$
    since we want the filter to predict the next sample of the random
    process $x(k)$
  \item computes the filter coefficients that will be used at the next
    time instant $\vec{c}(k+1) = \vec{c}_k + \mu e(k) \vec{x}_k^*$
\end{itemize}
This is repeated until time $k = K - 2$ since the last known sample of
the input $x(K-1)$ is needed to compute the value of the error
function $e(K-2)$.

We choose $N = 2$ as the order of the filter like in problem 3 and we
also use the realization of $x(k)$ generated with the same seed
value. The remaining parameter $\mu$ must be in the interval $
\left[0,\frac{2}{Nr_x(0)}\right] $ to ensure the convergence in the
mean square sense of $\vec{c}_{opt}$ and $J(k)$. The value we choose
is $\mu_{opt} = \frac{1}{Nr_x(0)}$ since it is the one that provides
the maximum convergence rate of $J(k)$ when $J(k) \gg J_{min}$. For
the signal power $r_x(0)$ we use the theoretical value, but the
unbiased estimator used in the previous problems would yield similar
results.

At the last iteration the filter coefficients assume the following
values
\begin{align*}
  c_1(K-1) &= 1.0071  - 0.2479j \\
  c_2(K-1) &= -0.8942 + 0.4401j
\end{align*}
these values are very close to the ones computed using the Wiener-Hopf
solution at problem 3. The real and imaginary parts of the first two
coefficients $c_1(k)$ and $c_2(k)$ are plotted as $k$ varies in
Fig.~\ref{plot:coeff_lms_c1} and Fig.~\ref{plot:coeff_lms_c2} where we
can notice that after $k= ...$ they are practically at the convergence
values.
%% \begin{table}[h]
%%   \centering
%%   \begin{tabular}{>{$}c<{$}>{$}c<{$}>{$}c<{$}}
%%     \text{Coefficient} & \text{Real part} & \text{Imaginary part} \\
%%     \hline
%%     c_0 & 1.0071 & - 0.2479j \\
%%     c_1 & -0.8942 & 0.4401j
%%   \end{tabular}
%%   \caption{Values of the LMS predictor coefficients at convegence}
%%   \label{tab:coeff_lms}
%% \end{table}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c1}
  \caption{Real and imaginary parts of $c_1(k)$}
  \label{plot:coeff_lms_c1}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/coeff_lms_c2}
  \caption{Real and imaginary parts of $c_2(k)$}
  \label{plot:coeff_lms_c2}
\end{figure}
In Fig.~\ref{plot:lms_mse} we compare the error function $e(k)$
computed in this realization with an average over 200 realizations.
Along with $\abs{e(k)}^2$ and $\E{ \abs{e(k)}^2 }$ we plotted the minimum MSE
$J_{min} = \sigma^2_d + \vec{p}^Hc_{opt}$ obtained when using the
Wiener-Hopf solution, and the value that we expect as $k$ goes to
infinity from equation 3.115 of the textbook:
\[ J(\infty) = \frac{2}{2-\mu N r_x(0)}J_{min} \]
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{matlab/lms_mse}
  \caption{LMS MSE}
  \label{plot:lms_mse}
\end{figure}
\section*{Problem 5}
\end{document}
